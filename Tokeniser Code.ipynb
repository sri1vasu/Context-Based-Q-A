{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a212a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "from transformers import Pipeline, PreTrainedTokenizer\n",
    "\n",
    "from transformers.utils import is_tf_available\n",
    "\n",
    "if is_tf_available():\n",
    "    import tensorflow as tf\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "END_KEY = \"### End\"\n",
    "INTRO_BLURB = (\n",
    "    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    ")\n",
    "\n",
    "# This is the prompt that is used for generating responses using an already trained model.  It ends with the response\n",
    "# key, where the job of the model is to provide the completion that follows it (i.e. the response itself).\n",
    "PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "{response_key}\n",
    "\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    ")\n",
    "\n",
    "\n",
    "def get_special_token_id(tokenizer: PreTrainedTokenizer, key: str) -> int:\n",
    "    \"\"\"Gets the token ID for a given string that has been added to the tokenizer as a special token.\n",
    "    When training, we configure the tokenizer so that the sequences like \"### Instruction:\" and \"### End\" are\n",
    "    treated specially and converted to a single, new token.  This retrieves the token ID each of these keys map to.\n",
    "    Args:\n",
    "        tokenizer (PreTrainedTokenizer): the tokenizer\n",
    "        key (str): the key to convert to a single token\n",
    "    Raises:\n",
    "        RuntimeError: if more than one ID was generated\n",
    "    Returns:\n",
    "        int: the token ID for the given key\n",
    "    \"\"\"\n",
    "    token_ids = tokenizer.encode(key)\n",
    "    if len(token_ids) > 1:\n",
    "        raise ValueError(f\"Expected only a single token for '{key}' but found {token_ids}\")\n",
    "    return token_ids[0]\n",
    "\n",
    "\n",
    "class InstructionTextGenerationPipeline(Pipeline):\n",
    "    def __init__(\n",
    "        self, *args, do_sample: bool = True, max_new_tokens: int = 256, top_p: float = 0.92, top_k: int = 0, **kwargs\n",
    "    ):\n",
    "        \"\"\"Initialize the pipeline\n",
    "        Args:\n",
    "            do_sample (bool, optional): Whether or not to use sampling. Defaults to True.\n",
    "            max_new_tokens (int, optional): Max new tokens after the prompt to generate. Defaults to 128.\n",
    "            top_p (float, optional): If set to float < 1, only the smallest set of most probable tokens with\n",
    "                probabilities that add up to top_p or higher are kept for generation. Defaults to 0.92.\n",
    "            top_k (int, optional): The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "                Defaults to 0.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, do_sample=do_sample, max_new_tokens=max_new_tokens, top_p=top_p, top_k=top_k,\n",
    "                         **kwargs)\n",
    "\n",
    "    def _sanitize_parameters(self,\n",
    "                             return_full_text: bool = None,\n",
    "                             **generate_kwargs):\n",
    "        preprocess_params = {}\n",
    "\n",
    "        # newer versions of the tokenizer configure the response key as a special token.  newer versions still may\n",
    "        # append a newline to yield a single token.  find whatever token is configured for the response key.\n",
    "        tokenizer_response_key = next(\n",
    "            (token for token in self.tokenizer.additional_special_tokens if token.startswith(RESPONSE_KEY)), None\n",
    "        )\n",
    "\n",
    "        response_key_token_id = None\n",
    "        end_key_token_id = None\n",
    "        if tokenizer_response_key:\n",
    "            try:\n",
    "                response_key_token_id = get_special_token_id(self.tokenizer, tokenizer_response_key)\n",
    "                end_key_token_id = get_special_token_id(self.tokenizer, END_KEY)\n",
    "\n",
    "                # Ensure generation stops once it generates \"### End\"\n",
    "                generate_kwargs[\"eos_token_id\"] = end_key_token_id\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        forward_params = generate_kwargs\n",
    "        postprocess_params = {\n",
    "            \"response_key_token_id\": response_key_token_id,\n",
    "            \"end_key_token_id\": end_key_token_id\n",
    "        }\n",
    "\n",
    "        if return_full_text is not None:\n",
    "            postprocess_params[\"return_full_text\"] = return_full_text\n",
    "\n",
    "        return preprocess_params, forward_params, postprocess_params\n",
    "\n",
    "    def preprocess(self, instruction_text, **generate_kwargs):\n",
    "        prompt_text = PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction_text)\n",
    "        inputs = self.tokenizer(\n",
    "            prompt_text,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs[\"prompt_text\"] = prompt_text\n",
    "        inputs[\"instruction_text\"] = instruction_text\n",
    "        return inputs\n",
    "\n",
    "    def _forward(self, model_inputs, **generate_kwargs):\n",
    "        input_ids = model_inputs[\"input_ids\"]\n",
    "        attention_mask = model_inputs.get(\"attention_mask\", None)\n",
    "\n",
    "        if input_ids.shape[1] == 0:\n",
    "            input_ids = None\n",
    "            attention_mask = None\n",
    "            in_b = 1\n",
    "        else:\n",
    "            in_b = input_ids.shape[0]\n",
    "\n",
    "        generated_sequence = self.model.generate(\n",
    "            input_ids=input_ids.to(self.model.device),\n",
    "            attention_mask=attention_mask.to(self.model.device) if attention_mask is not None else None,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            **generate_kwargs,\n",
    "        )\n",
    "\n",
    "        out_b = generated_sequence.shape[0]\n",
    "        if self.framework == \"pt\":\n",
    "            generated_sequence = generated_sequence.reshape(in_b, out_b // in_b, *generated_sequence.shape[1:])\n",
    "        elif self.framework == \"tf\":\n",
    "            generated_sequence = tf.reshape(generated_sequence, (in_b, out_b // in_b, *generated_sequence.shape[1:]))\n",
    "\n",
    "        instruction_text = model_inputs.pop(\"instruction_text\")\n",
    "        return {\"generated_sequence\": generated_sequence, \"input_ids\": input_ids, \"instruction_text\": instruction_text}\n",
    "\n",
    "    def postprocess(self, model_outputs, response_key_token_id, end_key_token_id, return_full_text: bool = False):\n",
    "\n",
    "        generated_sequence = model_outputs[\"generated_sequence\"][0]\n",
    "        instruction_text = model_outputs[\"instruction_text\"]\n",
    "\n",
    "        generated_sequence: List[List[int]] = generated_sequence.numpy().tolist()\n",
    "        records = []\n",
    "        for sequence in generated_sequence:\n",
    "\n",
    "            # The response will be set to this variable if we can identify it.\n",
    "            decoded = None\n",
    "\n",
    "            # If we have token IDs for the response and end, then we can find the tokens and only decode between them.\n",
    "            if response_key_token_id and end_key_token_id:\n",
    "                # Find where \"### Response:\" is first found in the generated tokens.  Considering this is part of the\n",
    "                # prompt, we should definitely find it.  We will return the tokens found after this token.\n",
    "                try:\n",
    "                    response_pos = sequence.index(response_key_token_id)\n",
    "                except ValueError:\n",
    "                    logger.warn(f\"Could not find response key {response_key_token_id} in: {sequence}\")\n",
    "                    response_pos = None\n",
    "\n",
    "                if response_pos:\n",
    "                    # Next find where \"### End\" is located.  The model has been trained to end its responses with this\n",
    "                    # sequence (or actually, the token ID it maps to, since it is a special token).  We may not find\n",
    "                    # this token, as the response could be truncated.  If we don't find it then just return everything\n",
    "                    # to the end.  Note that even though we set eos_token_id, we still see the this token at the end.\n",
    "                    try:\n",
    "                        end_pos = sequence.index(end_key_token_id)\n",
    "                    except ValueError:\n",
    "                        end_pos = None\n",
    "\n",
    "                    decoded = self.tokenizer.decode(sequence[response_pos + 1 : end_pos]).strip()\n",
    "\n",
    "            if not decoded:\n",
    "                # Otherwise we'll decode everything and use a regex to find the response and end.\n",
    "\n",
    "                fully_decoded = self.tokenizer.decode(sequence)\n",
    "\n",
    "                # The response appears after \"### Response:\".  The model has been trained to append \"### End\" at the\n",
    "                # end.\n",
    "                m = re.search(r\"#+\\s*Response:\\s*(.+?)#+\\s*End\", fully_decoded, flags=re.DOTALL)\n",
    "\n",
    "                if m:\n",
    "                    decoded = m.group(1).strip()\n",
    "                else:\n",
    "                    # The model might not generate the \"### End\" sequence before reaching the max tokens.  In this case,\n",
    "                    # return everything after \"### Response:\".\n",
    "                    m = re.search(r\"#+\\s*Response:\\s*(.+)\", fully_decoded, flags=re.DOTALL)\n",
    "                    if m:\n",
    "                        decoded = m.group(1).strip()\n",
    "                    else:\n",
    "                        logger.warn(f\"Failed to find response in:\\n{fully_decoded}\")\n",
    "\n",
    "            # If the full text is requested, then append the decoded text to the original instruction.\n",
    "            # This technically isn't the full text, as we format the instruction in the prompt the model has been\n",
    "            # trained on, but to the client it will appear to be the full text.\n",
    "            if return_full_text:\n",
    "                decoded = f\"{instruction_text}\\n{decoded}\"\n",
    "\n",
    "            rec = {\"generated_text\": decoded}\n",
    "\n",
    "            records.append(rec)\n",
    "\n",
    "        return records\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
